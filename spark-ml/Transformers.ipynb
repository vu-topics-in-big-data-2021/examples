{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers",
      "provenance": [],
      "authorship_tag": "ABX9TyPB5QQaUInlLNHqZ4bN1XPk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2021/examples/blob/main/spark-ml/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3gWGGHkDv8G"
      },
      "source": [
        "#Binarization is the process of thresholding numerical features to binary (0/1) features.\n",
        "#https://spark.apache.org/docs/3.1.1/ml-features.html#binarizer"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI_Ei_4FD8iv",
        "outputId": "c5dcd718-86ed-4143-a19e-389c3f51ed33"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark//spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.1.1-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 446740\n",
            "drwxr-xr-x  1 root root      4096 Apr 21 13:39 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Feb 22 02:11 spark-3.1.1-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFMO0GhwECSH",
        "outputId": "86e02275-45df-42ac-8fde-b9efcd85d6ff"
      },
      "source": [
        "from pyspark.ml.feature import Binarizer\n",
        "\n",
        "continuousDataFrame = spark.createDataFrame([\n",
        "    (0, 0.1),\n",
        "    (1, 0.8),\n",
        "    (2, 0.2)\n",
        "], [\"id\", \"feature\"])\n",
        "\n",
        "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
        "\n",
        "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
        "\n",
        "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
        "binarizedDataFrame.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binarizer output with Threshold = 0.500000\n",
            "+---+-------+-----------------+\n",
            "| id|feature|binarized_feature|\n",
            "+---+-------+-----------------+\n",
            "|  0|    0.1|              0.0|\n",
            "|  1|    0.8|              1.0|\n",
            "|  2|    0.2|              0.0|\n",
            "+---+-------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDCX0Q0DEQu9"
      },
      "source": [
        "#Another interesting transformer is PCA\n",
        "#PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeJodiZOEWv6"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "# $example on$\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R59GX33HEo0U"
      },
      "source": [
        "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
        "            (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
        "            (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
        "df = spark.createDataFrame(data, [\"features\"])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKcPl6hjEvws",
        "outputId": "26f76604-ab67-45e5-bba5-33d425ed72b7"
      },
      "source": [
        "df.show(truncate=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+\n",
            "|features             |\n",
            "+---------------------+\n",
            "|(5,[1,3],[1.0,7.0])  |\n",
            "|[2.0,0.0,3.0,4.0,5.0]|\n",
            "|[4.0,0.0,0.0,6.0,7.0]|\n",
            "+---------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNGJofsE1m_"
      },
      "source": [
        "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzLH989oE7Vb"
      },
      "source": [
        "model = pca.fit(df)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atHnHXKxFd1O",
        "outputId": "51718e14-438c-4c9d-cdec-6a5bb4c2f837"
      },
      "source": [
        "#select specific output and transform the dataframe\n",
        "result = model.transform(df).select([\"features\",\"pcaFeatures\"])\n",
        "result.explain()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Project [features#356, UDF(features#356) AS pcaFeatures#368]\n",
            "+- *(1) Scan ExistingRDD[features#356]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvMasOfCFl4J",
        "outputId": "ef952726-4bda-49b1-858f-33148afdb848"
      },
      "source": [
        "result.show(truncate=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+-----------------------------------------------------------+\n",
            "|features             |pcaFeatures                                                |\n",
            "+---------------------+-----------------------------------------------------------+\n",
            "|(5,[1,3],[1.0,7.0])  |[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
            "|[2.0,0.0,3.0,4.0,5.0]|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
            "|[4.0,0.0,0.0,6.0,7.0]|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
            "+---------------------+-----------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DfJYYkYF3dn"
      },
      "source": [
        "# An important transformer is discrete consine transofm\n",
        "#The Discrete Cosine Transform transforms a length N real-valued sequence in the time domain into another length N real-valued sequence in the frequency domain. "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-gbOUfzF_cj",
        "outputId": "7b0a5407-fdb5-4d9f-c1ab-f95a11a6b484"
      },
      "source": [
        "from pyspark.ml.feature import DCT\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (Vectors.dense([0.0, 1.0, -2.0, 3.0]),),\n",
        "    (Vectors.dense([-1.0, 2.0, 4.0, -7.0]),),\n",
        "    (Vectors.dense([14.0, -2.0, -5.0, 1.0]),)], [\"features\"])\n",
        "\n",
        "dct = DCT(inverse=False, inputCol=\"features\", outputCol=\"featuresDCT\")\n",
        "\n",
        "dctDf = dct.transform(df)\n",
        "\n",
        "dctDf.select([\"features\",\"featuresDCT\"]).show(truncate=False)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------------------------------------------------------+\n",
            "|features            |featuresDCT                                                     |\n",
            "+--------------------+----------------------------------------------------------------+\n",
            "|[0.0,1.0,-2.0,3.0]  |[1.0,-1.1480502970952693,2.0000000000000004,-2.7716385975338604]|\n",
            "|[-1.0,2.0,4.0,-7.0] |[-1.0,3.378492794482933,-7.000000000000001,2.9301512653149677]  |\n",
            "|[14.0,-2.0,-5.0,1.0]|[4.0,9.304453421915744,11.000000000000002,1.5579302036357163]   |\n",
            "+--------------------+----------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyJBwdsEGMjQ",
        "outputId": "8864c9dd-dee9-401e-c257-085e867cb0d9"
      },
      "source": [
        "#Another useful thing is string Indexer. \n",
        "from pyspark.ml.feature import StringIndexer, IndexToString\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
        "    [\"id\", \"category\"])\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
        "indexed = indexer.fit(df).transform(df)\n",
        "indexed.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-------------+\n",
            "| id|category|categoryIndex|\n",
            "+---+--------+-------------+\n",
            "|  0|       a|          0.0|\n",
            "|  1|       b|          2.0|\n",
            "|  2|       c|          1.0|\n",
            "|  3|       a|          0.0|\n",
            "|  4|       a|          0.0|\n",
            "|  5|       c|          1.0|\n",
            "+---+--------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6BPFIMYGWXC",
        "outputId": "ebe458d9-e976-48fa-e366-e7c5a856d83b"
      },
      "source": [
        "#Index to String is similar\n",
        "#Applying IndexToString with categoryIndex as the input column, originalCategory as the output column, we are able to retrieve our original labels (they will be inferred from the columnsâ€™ metadata):\n",
        "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
        "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
        "indexed.show()\n",
        "\n",
        "print(\"StringIndexer stores labels in output column metadata\\n\")\n",
        "\n",
        "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
        "converted = converter.transform(indexed)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformed string column 'category' to indexed column 'categoryIndex'\n",
            "+---+--------+-------------+\n",
            "| id|category|categoryIndex|\n",
            "+---+--------+-------------+\n",
            "|  0|       a|          0.0|\n",
            "|  1|       b|          2.0|\n",
            "|  2|       c|          1.0|\n",
            "|  3|       a|          0.0|\n",
            "|  4|       a|          0.0|\n",
            "|  5|       c|          1.0|\n",
            "+---+--------+-------------+\n",
            "\n",
            "StringIndexer stores labels in output column metadata\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4WF17L7GssI",
        "outputId": "6cf234e6-d98b-42dc-9b61-eee40c8ecaf2"
      },
      "source": [
        "\n",
        "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
        "converted = converter.transform(indexed)\n",
        "\n",
        "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
        "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
        "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
            "+---+-------------+----------------+\n",
            "| id|categoryIndex|originalCategory|\n",
            "+---+-------------+----------------+\n",
            "|  0|          0.0|               a|\n",
            "|  1|          2.0|               b|\n",
            "|  2|          1.0|               c|\n",
            "|  3|          0.0|               a|\n",
            "|  4|          0.0|               a|\n",
            "|  5|          1.0|               c|\n",
            "+---+-------------+----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0_Oc_agG2kA"
      },
      "source": [
        "#One Hot Encoding is another useful transformer\n",
        "#One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value \n",
        "#indicating the presence of a specific feature value from among the set of all feature values.\n",
        "#This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. \n",
        "#For string type input data, it is common to encode categorical features using StringIndexer first."
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AApv8Vt8HA_2",
        "outputId": "d4c198e5-4479-4b93-da6d-fd0e019c98ff"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (0.0, 1.0),\n",
        "    (1.0, 0.0),\n",
        "    (2.0, 1.0),\n",
        "    (0.0, 2.0),\n",
        "    (0.0, 1.0),\n",
        "    (2.0, 0.0)\n",
        "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
        "\n",
        "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
        "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
        "model = encoder.fit(df)\n",
        "encoded = model.transform(df)\n",
        "encoded.show()\n",
        "#For those not familiar with vector types in spark, the sparse vector seen in the third and fourth column below has 3 parts.\n",
        "#The first component which is a 0 indicates that it is a sparse vector. Not shown in the output\n",
        "#The second component talks about the size of the vector. The third component talks about the indices where the vector is populated while the fourth component talks about what values these are. \n",
        "#This truncates the vector and is really efficient when you have really large vector representations."
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+--------------+-------------+-------------+\n",
            "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
            "+--------------+--------------+-------------+-------------+\n",
            "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
            "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
            "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
            "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
            "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
            "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
            "+--------------+--------------+-------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MECbNgTnBM7",
        "outputId": "3538b475-3101-40b6-eb88-22a3edcae0bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.ml.linalg import DenseVector\n",
        "from pyspark.sql.types import FloatType,ArrayType\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "def toDense(v):\n",
        "  v = DenseVector(v)\n",
        "  new_array = list([float(x) for x in v])\n",
        "  return new_array \n",
        "denseudf=UserDefinedFunction(toDense, ArrayType(FloatType()))\n",
        "encoded.withColumn('categoryVecDense',denseudf('categoryVec1')).show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+--------------+-------------+-------------+----------------+\n",
            "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|categoryVecDense|\n",
            "+--------------+--------------+-------------+-------------+----------------+\n",
            "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|      [1.0, 0.0]|\n",
            "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|      [0.0, 1.0]|\n",
            "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|      [0.0, 0.0]|\n",
            "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|      [1.0, 0.0]|\n",
            "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|      [1.0, 0.0]|\n",
            "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|      [0.0, 0.0]|\n",
            "+--------------+--------------+-------------+-------------+----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDj8MPMymeAN"
      },
      "source": [
        "#in the above "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofnb4oRZHPj9"
      },
      "source": [
        "#Normalizer is used transforms a dataset of Vector rows, normalizing each Vector to have unit norm.\n",
        "# It takes parameter p, which specifies the p-norm used for normalization. (p=2 by default.) \n",
        "# This normalization can help standardize your input data and improve the behavior of learning algorithms."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6soJISCkHwCa"
      },
      "source": [
        "from pyspark.ml.feature import Normalizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "dataFrame = spark.createDataFrame([\n",
        "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
        "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
        "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
        "], [\"id\", \"features\"])\n",
        "\n",
        "# Normalize each Vector using $L^1$ norm.\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
        "l1NormData = normalizer.transform(dataFrame)\n",
        "print(\"Normalized using L^1 norm\")\n",
        "l1NormData.show(truncate=False)\n",
        "\n",
        "# Normalize each Vector using $L^\\infty$ norm.\n",
        "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
        "print(\"Normalized using L^inf norm\")\n",
        "lInfNormData.show(truncate=False)\n",
        "\n",
        "\n",
        "# Normalize each Vector using $L^2$ norm.\n",
        "l2NormData = normalizer.transform(dataFrame, {normalizer.p: float(2)})\n",
        "print(\"Normalized using L^2 norm\")\n",
        "l2NormData.show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzGHMuWjINsd"
      },
      "source": [
        "#Bucketizer: Bucketizer transforms a column of continuous features to a column of feature buckets\n",
        "# it usesthe split parameter for mapping continuous features into buckets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_M--RteIWVP"
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
        "\n",
        "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
        "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
        "\n",
        "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
        "\n",
        "# Transform original data into its bucket index.\n",
        "bucketedData = bucketizer.transform(dataFrame)\n",
        "\n",
        "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
        "bucketedData.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}