{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linearregression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVvycYbQiYmDdDd4hPGHfw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2021/examples/blob/main/spark-ml/linearregression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RGMODPoApcH"
      },
      "source": [
        "#adapted from https://towardsdatascience.com/apache-spark-mllib-tutorial-ec6f1cb336a9 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh0z8vIk-f-Z"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKgrSkqC_VhR",
        "outputId": "ad52e715-096a-4ff9-8826-6380feab87bc"
      },
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark//spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.1.1-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 223372\n",
            "drwxr-xr-x  1 root root      4096 Apr 21 13:39 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Feb 22 02:11 spark-3.1.1-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4u-UNWY_juP"
      },
      "source": [
        "# w use the boston housing dataset\n",
        "!wget https://raw.githubusercontent.com/vu-topics-in-big-data-2021/examples/main/spark-ml/boston_housing.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt2gjHCxATqE",
        "outputId": "68924252-269b-42d1-ab61-d133454a05e3"
      },
      "source": [
        "data = spark.read.csv('./boston_housing.csv', header=True, inferSchema=True)\n",
        "data.show(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
            "|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio|     b|lstat|medv|\n",
            "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
            "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
            "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
            "|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
            "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6D6Yv4zAb9g",
        "outputId": "72001606-6c51-48b9-ac36-e85c334d9aaa"
      },
      "source": [
        "#its good to check the spark explain\n",
        "data.explain()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "FileScan csv [crim#16,zn#17,indus#18,chas#19,nox#20,rm#21,age#22,dis#23,rad#24,tax#25,ptratio#26,b#27,lstat#28,medv#29] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/boston_housing.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<crim:double,zn:double,indus:double,chas:int,nox:double,rm:double,age:double,dis:double,rad...\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYhy6J2vAgH3",
        "outputId": "2ac182e9-da14-445a-bea6-3a787b28fb2c"
      },
      "source": [
        "#Spark MLâ€™s algorithms expect the data to be represented in two columns: Features and Labels.\n",
        "#Features is an array of data points of all the features to be used for prediction. Labels contain the output label for each data point.\n",
        "# Use Vector Assembler to create features\n",
        "feature_columns = data.columns[:-1]\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "help(VectorAssembler)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class VectorAssembler in module pyspark.ml.feature:\n",
            "\n",
            "class VectorAssembler(pyspark.ml.wrapper.JavaTransformer, pyspark.ml.param.shared.HasInputCols, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.param.shared.HasHandleInvalid, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
            " |  VectorAssembler(*, inputCols=None, outputCol=None, handleInvalid='error')\n",
            " |  \n",
            " |  A feature transformer that merges multiple columns into a vector column.\n",
            " |  \n",
            " |  .. versionadded:: 1.4.0\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n",
            " |  >>> vecAssembler = VectorAssembler(outputCol=\"features\")\n",
            " |  >>> vecAssembler.setInputCols([\"a\", \"b\", \"c\"])\n",
            " |  VectorAssembler...\n",
            " |  >>> vecAssembler.transform(df).head().features\n",
            " |  DenseVector([1.0, 0.0, 3.0])\n",
            " |  >>> vecAssembler.setParams(outputCol=\"freqs\").transform(df).head().freqs\n",
            " |  DenseVector([1.0, 0.0, 3.0])\n",
            " |  >>> params = {vecAssembler.inputCols: [\"b\", \"a\"], vecAssembler.outputCol: \"vector\"}\n",
            " |  >>> vecAssembler.transform(df, params).head().vector\n",
            " |  DenseVector([0.0, 1.0])\n",
            " |  >>> vectorAssemblerPath = temp_path + \"/vector-assembler\"\n",
            " |  >>> vecAssembler.save(vectorAssemblerPath)\n",
            " |  >>> loadedAssembler = VectorAssembler.load(vectorAssemblerPath)\n",
            " |  >>> loadedAssembler.transform(df).head().freqs == vecAssembler.transform(df).head().freqs\n",
            " |  True\n",
            " |  >>> dfWithNullsAndNaNs = spark.createDataFrame(\n",
            " |  ...    [(1.0, 2.0, None), (3.0, float(\"nan\"), 4.0), (5.0, 6.0, 7.0)], [\"a\", \"b\", \"c\"])\n",
            " |  >>> vecAssembler2 = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\",\n",
            " |  ...    handleInvalid=\"keep\")\n",
            " |  >>> vecAssembler2.transform(dfWithNullsAndNaNs).show()\n",
            " |  +---+---+----+-------------+\n",
            " |  |  a|  b|   c|     features|\n",
            " |  +---+---+----+-------------+\n",
            " |  |1.0|2.0|null|[1.0,2.0,NaN]|\n",
            " |  |3.0|NaN| 4.0|[3.0,NaN,4.0]|\n",
            " |  |5.0|6.0| 7.0|[5.0,6.0,7.0]|\n",
            " |  +---+---+----+-------------+\n",
            " |  ...\n",
            " |  >>> vecAssembler2.setParams(handleInvalid=\"skip\").transform(dfWithNullsAndNaNs).show()\n",
            " |  +---+---+---+-------------+\n",
            " |  |  a|  b|  c|     features|\n",
            " |  +---+---+---+-------------+\n",
            " |  |5.0|6.0|7.0|[5.0,6.0,7.0]|\n",
            " |  +---+---+---+-------------+\n",
            " |  ...\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      VectorAssembler\n",
            " |      pyspark.ml.wrapper.JavaTransformer\n",
            " |      pyspark.ml.wrapper.JavaParams\n",
            " |      pyspark.ml.wrapper.JavaWrapper\n",
            " |      pyspark.ml.base.Transformer\n",
            " |      pyspark.ml.param.shared.HasInputCols\n",
            " |      pyspark.ml.param.shared.HasOutputCol\n",
            " |      pyspark.ml.param.shared.HasHandleInvalid\n",
            " |      pyspark.ml.param.Params\n",
            " |      pyspark.ml.util.Identifiable\n",
            " |      pyspark.ml.util.JavaMLReadable\n",
            " |      pyspark.ml.util.MLReadable\n",
            " |      pyspark.ml.util.JavaMLWritable\n",
            " |      pyspark.ml.util.MLWritable\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, inputCols=None, outputCol=None, handleInvalid='error')\n",
            " |      __init__(self, \\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\n",
            " |  \n",
            " |  setHandleInvalid(self, value)\n",
            " |      Sets the value of :py:attr:`handleInvalid`.\n",
            " |  \n",
            " |  setInputCols(self, value)\n",
            " |      Sets the value of :py:attr:`inputCols`.\n",
            " |  \n",
            " |  setOutputCol(self, value)\n",
            " |      Sets the value of :py:attr:`outputCol`.\n",
            " |  \n",
            " |  setParams(self, *, inputCols=None, outputCol=None, handleInvalid='error')\n",
            " |      setParams(self, \\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\n",
            " |      Sets params for this VectorAssembler.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  handleInvalid = Param(parent='undefined', name='handleInvalid', ...o d...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
            " |  \n",
            " |  clear(self, param)\n",
            " |      Clears a param from the param map if it has been explicitly set.\n",
            " |  \n",
            " |  copy(self, extra=None)\n",
            " |      Creates a copy of this instance with the same uid and some\n",
            " |      extra params. This implementation first calls Params.copy and\n",
            " |      then make a copy of the companion Java pipeline component with\n",
            " |      extra params. So both the Python wrapper and the Java pipeline\n",
            " |      component get copied.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      extra : dict, optional\n",
            " |          Extra parameters to copy to the new instance\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :py:class:`JavaParams`\n",
            " |          Copy of this instance\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
            " |  \n",
            " |  __del__(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.base.Transformer:\n",
            " |  \n",
            " |  transform(self, dataset, params=None)\n",
            " |      Transforms the input dataset with optional parameters.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
            " |          input dataset\n",
            " |      params : dict, optional\n",
            " |          an optional param map that overrides embedded params.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :py:class:`pyspark.sql.DataFrame`\n",
            " |          transformed dataset\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.param.shared.HasInputCols:\n",
            " |  \n",
            " |  getInputCols(self)\n",
            " |      Gets the value of inputCols or its default value.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCols:\n",
            " |  \n",
            " |  inputCols = Param(parent='undefined', name='inputCols', doc='input col...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
            " |  \n",
            " |  getOutputCol(self)\n",
            " |      Gets the value of outputCol or its default value.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
            " |  \n",
            " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.param.shared.HasHandleInvalid:\n",
            " |  \n",
            " |  getHandleInvalid(self)\n",
            " |      Gets the value of handleInvalid or its default value.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.param.Params:\n",
            " |  \n",
            " |  explainParam(self, param)\n",
            " |      Explains a single param and returns its name, doc, and optional\n",
            " |      default value and user-supplied value in a string.\n",
            " |  \n",
            " |  explainParams(self)\n",
            " |      Returns the documentation of all params with their optionally\n",
            " |      default values and user-supplied values.\n",
            " |  \n",
            " |  extractParamMap(self, extra=None)\n",
            " |      Extracts the embedded default param values and user-supplied\n",
            " |      values, and then merges them with extra values from input into\n",
            " |      a flat param map, where the latter value is used if there exist\n",
            " |      conflicts, i.e., with ordering: default param values <\n",
            " |      user-supplied values < extra.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      extra : dict, optional\n",
            " |          extra param values\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      dict\n",
            " |          merged param map\n",
            " |  \n",
            " |  getOrDefault(self, param)\n",
            " |      Gets the value of a param in the user-supplied param map or its\n",
            " |      default value. Raises an error if neither is set.\n",
            " |  \n",
            " |  getParam(self, paramName)\n",
            " |      Gets a param by its name.\n",
            " |  \n",
            " |  hasDefault(self, param)\n",
            " |      Checks whether a param has a default value.\n",
            " |  \n",
            " |  hasParam(self, paramName)\n",
            " |      Tests whether this instance contains a param with a given\n",
            " |      (string) name.\n",
            " |  \n",
            " |  isDefined(self, param)\n",
            " |      Checks whether a param is explicitly set by user or has\n",
            " |      a default value.\n",
            " |  \n",
            " |  isSet(self, param)\n",
            " |      Checks whether a param is explicitly set by user.\n",
            " |  \n",
            " |  set(self, param, value)\n",
            " |      Sets a parameter in the embedded param map.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
            " |  \n",
            " |  params\n",
            " |      Returns all params ordered by name. The default implementation\n",
            " |      uses :py:func:`dir` to get all attributes of type\n",
            " |      :py:class:`Param`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
            " |  \n",
            " |  read() from abc.ABCMeta\n",
            " |      Returns an MLReader instance for this class.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
            " |  \n",
            " |  load(path) from abc.ABCMeta\n",
            " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
            " |  \n",
            " |  write(self)\n",
            " |      Returns an MLWriter instance for this ML instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
            " |  \n",
            " |  save(self, path)\n",
            " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl79BFN0BAFY"
      },
      "source": [
        "#VectorAssembler is a transformer that combines a given list of columns into a single vector column. \n",
        "#It is useful for combining raw features and features generated by different feature transformers into a single feature vector,\n",
        "# in order to train ML models like logistic regression and decision trees. VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. \n",
        "# In each row, the values of the input columns will be concatenated into a vector in the specified order.\n",
        "\n",
        "# Note this is one of the examples of transformers available to work with features\n",
        "# https://spark.apache.org/docs/3.1.1/ml-features.html\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_columns,outputCol=\"features\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8elty7XYBJVj"
      },
      "source": [
        "data_2 = assembler.transform(data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eei6Fqv2BK1g",
        "outputId": "037c8b80-4d7a-403b-f280-c3a2f2aeae2f"
      },
      "source": [
        "data_2.show(3, truncate=False) #notice the new column features"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+-------------------------------------------------------------------------+\n",
            "|crim   |zn  |indus|chas|nox  |rm   |age |dis   |rad|tax|ptratio|b     |lstat|medv|features                                                                 |\n",
            "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+-------------------------------------------------------------------------+\n",
            "|0.00632|18.0|2.31 |0   |0.538|6.575|65.2|4.09  |1  |296|15.3   |396.9 |4.98 |24.0|[0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09,1.0,296.0,15.3,396.9,4.98]  |\n",
            "|0.02731|0.0 |7.07 |0   |0.469|6.421|78.9|4.9671|2  |242|17.8   |396.9 |9.14 |21.6|[0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,396.9,9.14] |\n",
            "|0.02729|0.0 |7.07 |0   |0.469|7.185|61.1|4.9671|2  |242|17.8   |392.83|4.03 |34.7|[0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671,2.0,242.0,17.8,392.83,4.03]|\n",
            "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+-------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkXAGq2WBXkq"
      },
      "source": [
        "#lets split the data\n",
        "train, test = data_2.randomSplit([0.7, 0.3])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfPTh5qzBdRH"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtdVQpgHBfNV"
      },
      "source": [
        "algo = LinearRegression(featuresCol=\"features\", labelCol=\"medv\") # label column is the predicted variable\n",
        " "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W7GFVnRB154"
      },
      "source": [
        "model = algo.fit(train) #thats it"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjbthLJ8CALk",
        "outputId": "94d02651-d4ea-4540-db11-1160624fe3d4"
      },
      "source": [
        "#evaluation\n",
        "evaluation_summary = model.evaluate(test)\n",
        "print(evaluation_summary.meanAbsoluteError,evaluation_summary.rootMeanSquaredError,evaluation_summary.r2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.316490447070998 4.92611343573216 0.7677108562100609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml4bPv1MCP96",
        "outputId": "5bb180c6-f188-40f7-b18e-1cfb362f1b58"
      },
      "source": [
        "#lets use the model \n",
        "predictions = model.transform(test)\n",
        "predictions.explain()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Project [crim#16, zn#17, indus#18, chas#19, nox#20, rm#21, age#22, dis#23, rad#24, tax#25, ptratio#26, b#27, lstat#28, medv#29, features#119, UDF(features#119) AS prediction#624]\n",
            "+- *(1) Sample 0.7, 1.0, false, 8423701554449180293\n",
            "   +- *(1) Sort [crim#16 ASC NULLS FIRST, zn#17 ASC NULLS FIRST, indus#18 ASC NULLS FIRST, chas#19 ASC NULLS FIRST, nox#20 ASC NULLS FIRST, rm#21 ASC NULLS FIRST, age#22 ASC NULLS FIRST, dis#23 ASC NULLS FIRST, rad#24 ASC NULLS FIRST, tax#25 ASC NULLS FIRST, ptratio#26 ASC NULLS FIRST, b#27 ASC NULLS FIRST, lstat#28 ASC NULLS FIRST, medv#29 ASC NULLS FIRST, features#119 ASC NULLS FIRST], false, 0\n",
            "      +- *(1) Project [crim#16, zn#17, indus#18, chas#19, nox#20, rm#21, age#22, dis#23, rad#24, tax#25, ptratio#26, b#27, lstat#28, medv#29, UDF(struct(crim, crim#16, zn, zn#17, indus, indus#18, chas_double_VectorAssembler_c355670d8a81, cast(chas#19 as double), nox, nox#20, rm, rm#21, age, age#22, dis, dis#23, rad_double_VectorAssembler_c355670d8a81, cast(rad#24 as double), tax_double_VectorAssembler_c355670d8a81, cast(tax#25 as double), ptratio, ptratio#26, b, b#27, ... 2 more fields)) AS features#119]\n",
            "         +- FileScan csv [crim#16,zn#17,indus#18,chas#19,nox#20,rm#21,age#22,dis#23,rad#24,tax#25,ptratio#26,b#27,lstat#28,medv#29] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/boston_housing.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<crim:double,zn:double,indus:double,chas:int,nox:double,rm:double,age:double,dis:double,rad...\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbjb0Zb6CcwO",
        "outputId": "f36ee5f2-338d-4d21-80d6-d0c3ef3c39d5"
      },
      "source": [
        "predictions.show(5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
            "|   crim|   zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio|     b|lstat|medv|            features|        prediction|\n",
            "+-------+-----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
            "|0.01301| 35.0| 1.52|   0|0.442|7.241|49.3|7.0379|  1|284|   15.5|394.74| 5.49|32.7|[0.01301,35.0,1.5...|29.463241795530195|\n",
            "|0.01311| 90.0| 1.22|   0|0.403|7.249|21.9|8.6966|  5|226|   17.9|395.93| 4.81|35.4|[0.01311,90.0,1.2...|  30.5662269377407|\n",
            "|0.01432|100.0| 1.32|   0|0.411|6.816|40.5|8.3248|  5|256|   15.1| 392.9| 3.95|31.6|[0.01432,100.0,1....|31.989791611768283|\n",
            "|0.01501| 90.0| 1.21|   1|0.401|7.923|24.8| 5.885|  1|198|   13.6|395.52| 3.16|50.0|[0.01501,90.0,1.2...|43.281543352728015|\n",
            "|0.01538| 90.0| 3.75|   0|0.394|7.454|34.2|6.3361|  3|244|   15.9|386.34| 3.11|44.0|[0.01538,90.0,3.7...|36.518594469759115|\n",
            "+-------+-----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}