{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decisiontreeclassifierAndCrossValidation",
      "provenance": [],
      "authorship_tag": "ABX9TyPPTl71iVBEtBTUGxYPcN4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2021/examples/blob/main/spark-ml/decisiontreeclassifierAndCrossValidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhYjtaNqMdTy"
      },
      "source": [
        "#https://spark.apache.org/docs/3.1.1/ml-classification-regression.html"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUWGK-szMiUj",
        "outputId": "5cce6006-ad6e-46fc-998f-1a3633a557be"
      },
      "source": [
        "#ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines with spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark//spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.1.1-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 223372\n",
            "drwxr-xr-x  1 root root      4096 Apr 21 13:39 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Feb 22 02:11 spark-3.1.1-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BLivRbZMzMT",
        "outputId": "7c31369d-14d7-4201-aece-b29e32c023ef"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/eBay/Spark/master/data/mllib/sample_libsvm_data.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-27 02:40:44--  https://raw.githubusercontent.com/eBay/Spark/master/data/mllib/sample_libsvm_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104736 (102K) [text/plain]\n",
            "Saving to: ‘sample_libsvm_data.txt’\n",
            "\n",
            "\rsample_libsvm_data.   0%[                    ]       0  --.-KB/s               \rsample_libsvm_data. 100%[===================>] 102.28K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-04-27 02:40:44 (34.9 MB/s) - ‘sample_libsvm_data.txt’ saved [104736/104736]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYt67ELtM1X_",
        "outputId": "0ee49ef8-9a78-48c9-aa7a-769144c735ce"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load the data stored in LIBSVM format as a DataFrame.\n",
        "data = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexers and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "\n",
        "treeModel = model.stages[2]\n",
        "# summary only\n",
        "print(treeModel)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------+--------------------+\n",
            "|prediction|indexedLabel|            features|\n",
            "+----------+------------+--------------------+\n",
            "|       1.0|         1.0|(692,[98,99,100,1...|\n",
            "|       1.0|         1.0|(692,[123,124,125...|\n",
            "|       1.0|         1.0|(692,[124,125,126...|\n",
            "|       1.0|         1.0|(692,[124,125,126...|\n",
            "|       1.0|         1.0|(692,[124,125,126...|\n",
            "+----------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0.105263 \n",
            "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_ca718c828fd8, depth=1, numNodes=3, numClasses=2, numFeatures=692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWhZQ0B8NTSW",
        "outputId": "1f1315ea-971a-482b-d792-348a9e0751f4"
      },
      "source": [
        "# Cross Validation Example\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Prepare training documents, which are labeled.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0),\n",
        "    (4, \"b spark who\", 1.0),\n",
        "    (5, \"g d a y\", 0.0),\n",
        "    (6, \"spark fly\", 1.0),\n",
        "    (7, \"was mapreduce\", 0.0),\n",
        "    (8, \"e spark program\", 1.0),\n",
        "    (9, \"a e c l\", 0.0),\n",
        "    (10, \"spark compile\", 1.0),\n",
        "    (11, \"hadoop software\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
        "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
        "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
        "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
        "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
        "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),\n",
        "                          numFolds=2)  # use 3+ folds in practice\n",
        "\n",
        "# Run cross-validation, and choose the best set of parameters.\n",
        "cvModel = crossval.fit(training)\n",
        "\n",
        "# Prepare test documents, which are unlabeled.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"mapreduce spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
        "prediction = cvModel.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    print(row)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Row(id=4, text='spark i j k', probability=DenseVector([0.3413, 0.6587]), prediction=1.0)\n",
            "Row(id=5, text='l m n', probability=DenseVector([0.9438, 0.0562]), prediction=0.0)\n",
            "Row(id=6, text='mapreduce spark', probability=DenseVector([0.3451, 0.6549]), prediction=1.0)\n",
            "Row(id=7, text='apache hadoop', probability=DenseVector([0.9561, 0.0439]), prediction=0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}