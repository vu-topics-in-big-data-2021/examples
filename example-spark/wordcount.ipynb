{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordcount",
      "provenance": [],
      "authorship_tag": "ABX9TyMydNITPgTwahkLP2gztX5N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2021/examples/blob/main/example-spark/wordcount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TqXvJnJjdLk"
      },
      "source": [
        "#A simple workd count example"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmHhdiqQho9u"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Exq5o6RiNr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2971af6-67c6-4bc5-f347-bf273e000ad5"
      },
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark//spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 223372\n",
            "drwxr-xr-x  1 root root      4096 Mar 18 13:36 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Feb 22 02:11 spark-3.1.1-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQiUr_sviyJb"
      },
      "source": [
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.1.1-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "staDohTqjcss"
      },
      "source": [
        "#The entry point to using Spark SQL is an object called SparkSession. It initiates a Spark Application which all the code for that Session will run on\n",
        "# to larn more see https://blog.knoldus.com/spark-why-should-we-use-sparksession/\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6JMROZnkIWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08805e3-d2b4-4da6-ca47-e20e4c3cc6d1"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/vu-topics-in-big-data-2021/examples/main/example-spark/data/file.txt\n",
        "!head -2 file.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Smart Public Transit - Transit Hub\n",
            "This project addresses the problem of urban transportation and congestion by building analytical tools that help the customers and the transit agencies reduce uncertainties and optimize the transit operations. We adress this problem at three fronts - Data Analytics, Planning and analysis tool for understanding and projecting the impact of transportation choices, and developing scalable data stores that can enable cities to operate their own data lakes and analytics engines. As part of the project we also created an application called Transit Hub. Recently, we have been looking at the endogenous uncertainties and costs of transit operations as part of the the energy optimization project.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CPqGnFikMpP"
      },
      "source": [
        "from operator import add\n",
        "lines = spark.read.text(\"file.txt\").rdd.map(lambda r: r[0])\n",
        "counts = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (x.lower().strip(\"'() ,-.\"), 1)).reduceByKey(add)\n",
        "output = counts.sortBy(lambda a: a[1],ascending=False)\n",
        "for (word, count) in output.collect():\n",
        "    print(\"%s: %i\" % (word, count))\n",
        "\n",
        "#spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}